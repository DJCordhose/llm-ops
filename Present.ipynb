{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOvR+8jdyTmPAwDVWFr97aJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/llm-ops/blob/main/Present.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Lingual Medical Assessment Classification\n",
        "\n",
        "## LLama 3.1 8B\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "\n",
        "### Measurements\n",
        "\n",
        "#### 4 Bit T4\n",
        "* Negatives: 35 s\n",
        "* Positives: 38 s\n",
        "\n",
        "#### 4 Bit T4 (optimized config)\n",
        "* Negatives: 24 s\n",
        "* Positives: 25 s\n",
        "\n",
        "#### 8 Bit T4\n",
        "* Negatives: 23 s\n",
        "* Positives: 23 s\n",
        "\n",
        "#### Full Res L4\n",
        "* Negatives: 12.5 s\n",
        "* Positives: 12.5 s\n",
        "\n",
        "## Phi 3.5 MoE\n",
        "* https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280\n",
        "* https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
        "* https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\n",
        "\n",
        "### Measurements\n",
        "\n",
        "#### 4 Bit A100\n",
        "* Negatives: 50 s\n",
        "* Positives: 50 s\n",
        "\n",
        "## Memory consumption and Quantization\n",
        "\n",
        "### Quantization 4-Bit / 8-Bit\n",
        "* 4-Bit (FP4): https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "  * computation is not done in 4bit, the weights and activations are compressed to that format and the computation is still kept in native dtype\n",
        "* 8-Bit (LLM.int8()): https://huggingface.co/blog/hf-bitsandbytes-integration#a-gentle-summary-of-llmint8-zero-degradation-matrix-multiplication-for-large-language-models\n",
        "  * performs the matrix multiplication of the outliers in FP16 and the non-outliers in int8\n",
        "\n",
        "### Memory consumption for Lllama 3.1 models\n",
        "- How much memory does Llama 3.1 need for weights and cache (depending on the actually used context length) https://huggingface.co/blog/llama31#inference-memory-requirements\n",
        "- Detailed, but still comprehensive explanation of how inference in LLMs works: https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n"
      ],
      "metadata": {
        "id": "iR6_IfMp4Pvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on: Inference on commodity hardware\n",
        "\n",
        "**Run the assessment classification on the largest Llama 3.1 model possible**\n",
        "\n",
        "1. Get a very subjective impression: Do you like the results in terms of e.g.\n",
        "   1. Quality of language\n",
        "   1. Halluzination\n",
        "   1. Accuracy\n",
        "   1. Speed\n",
        "   1. Memory consumption\n",
        "1. If more than one Llama 3.1 model actually runs, subjectively compare the results\n",
        "\n",
        "## Optional\n",
        "1. Compare to Phi Mini\n",
        "1. Try a different langauge (German is prepared)\n"
      ],
      "metadata": {
        "id": "Sn_aFrdGCLHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate flash_attn torch bitsandbytes"
      ],
      "metadata": {
        "id": "ruMEtKBS4o6g",
        "outputId": "28404f1f-653e-4154-fbd4-2a07dcfba7bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.5 ms, sys: 2.76 ms, total: 13.2 ms\n",
            "Wall time: 2.61 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "J5kBd8psB1or"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure HuggingFace token as a Colab Secret, use key symbol on the left panel\n",
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E93a_cgwBsPW",
        "outputId": "6406c09c-128b-4bd6-bde2-62a4fcd33146"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "y-SYiDVt4Oxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "pBA0hHok7bmG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pEQyik-rrhL",
        "outputId": "c8733b16-a1a7-41f2-d31e-6c9415b98465"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 26 08:21:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   50C    P8              17W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kind = 'Lllama_3.1_8B_4bit'\n",
        "# kind = 'Lllama_3.1_8B_8bit'\n",
        "kind = 'Lllama_3.1_8B_16bit'\n",
        "# kind = 'Phi-3.5-MoE_4bit'\n",
        "# kind = \"Phi-3.5-mini_16bit\"\n",
        "\n",
        "# lang = \"de\"\n",
        "lang = \"en\""
      ],
      "metadata": {
        "id": "guL3PrUnQ8PZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"Lllama_3.1_8B\" in kind:\n",
        "  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "  # model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "  # model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "elif \"Phi-3.5-MoE\" in kind:\n",
        "  model_id = \"microsoft/Phi-3.5-MoE-instruct\"\n",
        "else:\n",
        "  model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "print(model_id)"
      ],
      "metadata": {
        "id": "q5mV16Pt1NU6",
        "outputId": "636a7b08-b3da-4ff6-89b0-105985941b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta-llama/Meta-Llama-3.1-8B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "vYDzjGdHVXaG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***note:*** execute in a termial 'watch -n 0.5 nvidia-smi' to see the GPU usage and when the model is loaded onto it"
      ],
      "metadata": {
        "id": "miDHXD2eVMXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "torch_dtype = None\n",
        "quantization_config = None\n",
        "\n",
        "\n",
        "if \"8bit\" in kind:\n",
        "  print(\"Using 8Bit quantization\")\n",
        "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "elif \"4bit\" in kind:\n",
        "  print(\"Using 4Bit quantization\")\n",
        "  # quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "else:\n",
        "  print(\"Using Full Resolution\")\n",
        "  torch_dtype = torch.bfloat16\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True,\n",
        "    deb\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "SK7DWCkEsLdv",
        "outputId": "ecd582d0-4f11-4dcc-89d8-2834edb2b742"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "positional argument follows keyword argument (<unknown>, line 31)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-10-fc0566151407>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<cell line: 1>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\\nimport torch\\n\\ntorch_dtype = None\\nquantization_config = None\\n\\n\\nif \"8bit\" in kind:\\n  print(\"Using 8Bit quantization\")\\n  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\\nelif \"4bit\" in kind:\\n  print(\"Using 4Bit quantization\")\\n  # quantization_config = BitsAndBytesConfig(load_in_4bit=True)\\n  quantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n  )\\nelse:\\n  print(\"Using Full Resolution\")\\n  torch_dtype = torch.bfloat16\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch_dtype,\\n    device_map=\"cuda\",\\n    trust_remote_code=True,\\n    deb\\n)\\n')\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\"\u001b[0m, line \u001b[1;32m334\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    return super().run_cell_magic(magic_name, line, cell)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2473\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
            "  File \u001b[1;32m\"<decorator-gen-54>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1291\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "z8oNcTA51SAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_en = [\n",
        "  \"With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\",\n",
        "  \"The socio-medical prerequisites for the prescribed aid supply have been met.\",\n",
        "  \"Everyday relevant usage benefits have been determined.\",\n",
        "  \"Socio-medical indication for the aid is confirmed.\",\n",
        "  \"Contraindications have been excluded; there are no contraindications for the use of the requested aid.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-V0LQbkdTDoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_en = [\n",
        "  \"No specific findings can be derived from the diagnosis currently named as the basis for the regulation.\",\n",
        "  \"According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.\",\n",
        "  \"A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.\",\n",
        "  \"From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.\",\n",
        "  \"A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.\"\n",
        "]"
      ],
      "metadata": {
        "id": "8mu2wI-9TVYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_de = [\n",
        "  \"Bei der hier benannten Diagnose ist das Erfordernis eines Ausgleichs zur Sicherstellung des Grundbedürfnisses denkbar.\",\n",
        "  \"Die sozialmedizinischen Voraussetzungen für die verordnete Hilfsmittelversorgung sind erfüllt.\",\n",
        "  \"Alltagsrelevante Gebrauchsvorteile werden festgestellt.\",\n",
        "  \"Sozialmedizinische Indikation für das Hilfsmittel wird bestätigt.\",\n",
        "  \"Kontraindikationen wurden ausgeschlossen, es liegen keine Gegenanzeigen für die Verwendung des beantragten Hilfsmittels vor.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PkFlXrwuuH_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_de = [\n",
        "  \"Aus der aktuell als verordnungsbegründend benannten Diagnose lässt sich kein konkreter Befund ableiten.\",\n",
        "  \"Gemäß den Leistungsauszügen der Krankenkasse ist der Versicherte bereits entsprechend dem Einsatzbereich des beantragten funktionellen Produkt versorgt.\",\n",
        "  \"Eine medizinisch nachvollziehbare Begründung, weshalb der Einsatz einer befundadäquaten orthopädietechnischen Hilfsmittelversorgung nicht ausreichend und stattdessen eine elektrische Fußheberstimulation zum Gehen zweckmäßiger und deshalb notwendig wäre, wurde nicht übermittelt.\",\n",
        "  \"In der Gesamtschau der hier vorliegenden Informationen kann nicht erkannt werden, wie die Versorgung des Versicherten mit dem Produkt begründet werden könnte, noch kann die Unbedenklichkeit einer solchen Versorgung bestätigt werden.\",\n",
        "  \"Eine ärztliche Begründung, warum im vorliegenden Fall ein nicht im Hilfsmittelverzeichnis gelistetes Produkt zum Einsatz kommen soll, wird nicht übermittelt.\"\n",
        "]"
      ],
      "metadata": {
        "id": "K2D3Y5K1uJpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if lang == \"de\":\n",
        "  negative = negative_de\n",
        "  positive = positive_de\n",
        "else:\n",
        "  negative = negative_en\n",
        "  positive = positive_en\n",
        "\n"
      ],
      "metadata": {
        "id": "8Mg3YZG2t-Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assessment = negative[0]\n",
        "# assessment = positive[0]"
      ],
      "metadata": {
        "id": "YGUixjbJTa6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if lang == \"de\":\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Du bist ein kompetenter Experte auf dem Gebiet der gesetzlichen Krankenversicherung und sprichst Deutsch. Antworte präzise, ernst und formell.\"},\n",
        "    {\"role\": \"user\", \"content\": f'''\n",
        "    Was ist das Ergebnis der Bewertung? Wird eine positive oder negative Empfehlung gegeben? Antworte mit 'Ja' oder 'Nein' und gib anschließend eine sehr kurze Begründung für die Einschätzung.\"\n",
        "\n",
        "    # Assessment\n",
        "    {assessment}\n",
        "\n",
        "    '''}\n",
        "  ]\n",
        "else:\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of statutory health insurance. Answer consice, serious and formal.\"},\n",
        "    {\"role\": \"user\", \"content\": f'''\n",
        "What is the result of the assessment? Is a positive or negative recommendation given? Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\n",
        "\n",
        "# Assessment\n",
        "{assessment}\n",
        "\n",
        "'''}\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=terminators,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=False\n",
        ")\n",
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "Markdown(tokenizer.decode(response, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "CoxgSA_Xz0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_assessment(assessment):\n",
        "  if lang == \"de\":\n",
        "    yes = \"Ja\"\n",
        "    no = \"Nein\"\n",
        "    messages = [\n",
        "  {\"role\": \"system\", \"content\": \"Du bist ein kompetenter Experte auf dem Gebiet der gesetzlichen Krankenversicherung und sprichst Deutsch. Antworte präzise, ernst und formell.\"},\n",
        "  {\"role\": \"user\", \"content\": f'''\n",
        "  Was ist das Ergebnis der Bewertung? Wird eine positive oder negative Empfehlung gegeben? Antworte mit 'Ja' oder 'Nein' und gib anschließend eine sehr kurze Begründung für die Einschätzung.\"\n",
        "\n",
        "  # Assessment\n",
        "  {assessment}\n",
        "\n",
        "  '''}\n",
        "  ]\n",
        "  else:\n",
        "    yes = \"Yes\"\n",
        "    no = \"No\"\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of statutory health insurance. Answer consice, serious and formal.\"},\n",
        "      {\"role\": \"user\", \"content\": f'''\n",
        "  What is the result of the assessment? Is a positive or negative recommendation given? Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\n",
        "\n",
        "  # Assessment\n",
        "  {assessment}\n",
        "\n",
        "  '''}\n",
        "  ]\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=512,\n",
        "      eos_token_id=terminators,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False\n",
        "  )\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  result = tokenizer.decode(response, skip_special_tokens=True)\n",
        "  if result.startswith(yes):\n",
        "    return \"Positive\", result\n",
        "  elif result.startswith(no):\n",
        "    return \"Negative\", result\n",
        "  else:\n",
        "    return \"Neutral\", result"
      ],
      "metadata": {
        "id": "kS5CfvlVTSjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative"
      ],
      "metadata": {
        "id": "reHbxlx1VGcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "negative_results = []\n",
        "negative_explanations = []\n",
        "\n",
        "for assessment in negative:\n",
        "  print(f\"Assessment: {assessment}\")\n",
        "  result, explanation = eval_assessment(assessment)\n",
        "  negative_results.append(result)\n",
        "  negative_explanations.append(explanation)\n",
        "  print(f\"{result}: {explanation}\")\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "id": "otADLjgLTwZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive"
      ],
      "metadata": {
        "id": "2bBV2H-UVIuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "positive_results = []\n",
        "positive_explanations = []\n",
        "\n",
        "for assessment in positive:\n",
        "  print(f\"Assessment: {assessment}\")\n",
        "  result, explanation = eval_assessment(assessment)\n",
        "  positive_results.append(result)\n",
        "  positive_explanations.append(explanation)\n",
        "  print(f\"{result}: {explanation}\")\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "id": "w3jE_bwWVJlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WZr-hfVH6Nsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'assesment': negative + positive,\n",
        "    'y_true': ['Negative'] * len(negative) + ['Positive'] * len(positive),\n",
        "    'y_hat': negative_results + positive_results,\n",
        "    'explanation': negative_explanations + positive_explanations\n",
        "})\n",
        "df"
      ],
      "metadata": {
        "id": "ha2oxPvtNBLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(f'results_{kind}_{lang}.xlsx', index=False)"
      ],
      "metadata": {
        "id": "Qx92Kvt2NtEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "id": "PKTEoSfxOKIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}